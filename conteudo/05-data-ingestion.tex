\newcommand{\sla}{\textbackslash}

\newcommand{\cmd}[1]{\textsf{#1}}

\newcommand{\pkg}[1]{\textsf{#1}}

\newcommand{\ltxcmd}[1]{\cmd{\sla{}#1}}

\chapter{Data Ingestion Study Case}
\label{chap:dataingestion}

\section{Introduction}
\label{sec:introductioning}
OpenDataHub (ODH) is a digital platform that makes data from different sources available in a standardized pattern, removing the complexity of integrating multiple data sources for the users\footnote{Reference from OpenDataHub's webpage https://opendatahub.com/}. The full picture of their system can be split into three functionalities:

\begin{itemize}
    \item Data ingestion: applications responsible for retrieving or receiving data from external data sources, identifying, transforming, and storing it.
    \item Data storage: the data is stored in a stations-values format  – stations are complex objects that have a type and can contain other stations or properties, a list of numeric values followed by data that describe it, such as value name and measurement unit\footnote{Reference OpenDataHub's documentation https://opendatahub.readthedocs.io/en/latest/howto/mobility/getstarted.html#getting-started}. For example, a charging plug station is a station that contains plugs, each plug is a station that contains a value ”available” or “not available”. Stations can contain more than one value, for example, a latitude-longitude point is a station that contains values for wind speed, wind direction, air pressure, and others. These values are stored in the same unit for every station, no matter the original data source unit and format.
    \item Data exposure: a common API to make data from stations available for end users.
\end{itemize}

This case study focuses on the data ingestion architecture, and how to increase its extensibility.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{collector-consumer-CONTEXT.drawio}
    \caption{Data Ingestion context diagram.\label{fig:subfigures9}}
\end{figure}

\section{Current architecture}
\label{sec:current}

The ODH's data collectors architecture's initial requirement is to connect to different data sources, using different protocols (e.g. HTTP APIs, MQTT, FTP) to download the data, aggregate and transform it to the stations-value pattern, and save it on ODH’s storage.

The current architecture uses independent applications dedicated to each data source. Each application is responsible for connecting to the data source, retrieving the data, aggregating and/or transforming it, and sending it to ODH's system for persisting data.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{collector-consumer-OLD CONTAINERS.drawio}
    \caption{Data Ingestion current architecture containers diagram.\label{fig:subfigures10}}
\end{figure}

This solution contains a problem of redundant work for each data collector service: they all have code to connect with the ODH's persistence system and might benefit from reusing processing and transformation tooling. This creates N dependencies with the storage, consequently, if there is a change in how the connection with the ODH's persistence system is made, it would require changing every data collector.

Besides, to integrate a new data source into the data collection, it is necessary to create a new service for collecting, parsing, transforming, and persisting code.

\section{New requirements}
\label{sec:requirements}

As a growing platform, ODH wants to be able to integrate new data sources with ease, decreasing technical requirements and development time. As part of this, it is essential to provide a way for data sources' teams to integrate with ODH's data collection, without requiring (or at least reducing) ODH's technical team involvement.

Furthermore, it is essential to have a way to track and debug the ingestion flow. For this reason, another requirement is to integrate a new database to store the “raw” data, which means, the data as it is collected before it is transformed into the ODH stations pattern.

For this, the current architecture would require each data collector application to connect to the raw data storage and store it. Creating another dependency coupling problem, since every data collector would require a dependency with the Raw data storage.

\section{Proposed architecture}
\label{sec:proposed}

The new architecture proposed in this thesis aims to increase the system's extensibility by reducing development time to create new data collectors. With it, it creates an option for data sources that want to integrate in, reducing ODH's technical team involvement and removing data collectors coupling with storage systems.

The new architecture uses microservices and asynchronous communication to modularize and split concerns along the system. (Figure~\ref{fig:containers})

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{collector-consumer-CONTAINERS.drawio}
    \caption{Data Ingestion proposed architecture containers diagram.\label{fig:containers}}
\end{figure}

The biggest challenge in this architecture is: How to treat external data and their singularities. Each data source has its data format and structure, how to extract the information needed by ODH and transform it into the expected pattern?

It can be solved with structural metadata, metadata is a kind of data used to describe data, in this case, it will contain information about the data structure: in which field is an important value, what are the data types, and how to access each wanted value.

This metadata can be sent with the data or configured, in case the same metadata can be used multiple times and is needed for the transformation process.

\subsection{Ingestion}
\label{sec:ingestion}

The data ingestion is done by the Ingestion Service. It is responsible for receiving the raw data from data collectors or active data sources (data sources sending the data directly to it), persisting this data into a raw data storage, and sending data to be transformed.

Data can only be sent to be processed once there is a metadata configuration to it. This metadata can be sent with the data, or be fetched from the Metadata Service (application responsible for handling metadata operations).

However, the raw data persistence does not depend on the metadata configuration. This means that data received without metadata or any metadata configuration will be persisted on Raw Data Storage and sent for transformation once the metadata is configured. For this reason, there is a dedicated worker to listen for metadata update messages and trigger this process. (Figure~\ref{fig:ingest})

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{collector-consumer-COMPONENT INGEST.drawio}
    \caption{Ingestion service components diagram.\label{fig:ingest}}
\end{figure}

\subsection{Metadata}
\label{sec:metadata}

The solution uses structural metadata about the data ingested to transform data from its raw structure into the ODH's station pattern. This metadata can be sent or collected with the data or can be configured to be applied to data collected during a time range(~\cite{Metadata}).

There is a metadata storage to store active configurations and to help debugging. The application dedicated to handling access and operations on the storage is the Metadata Service.

There is also a Metadata Configuration Interface to help developers and data sources configure new metadata and manage their data configuration. (Figure~\ref{fig:metadata})

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{collector-consumer-COMPONENT METADATA.drawio}
    \caption{Metadata service components diagram.\label{fig:metadata}}
\end{figure}

The metadata is built with a common structure and stored as a JSON. Follows an example of metadata in Program~\ref{prog:metadata}.

\begin{program}
    \index{JSON}
    \centering
  
  \begin{lstlisting}[language=JSON, style=wider]
{
 "sourceId": "austrian-geosphere",
 "startDate": "2015-08-04T10:11:30",
 "expirationDate": "2029-08-04T10:11:30",
 "metadata": {
 "dataId": "austrian-geosphere-data",
 "origin": "austrian-geosphere-data-collector",
 "period": 600,
 "timestampProperty": "timestamps.0",
 "results": {
 "type": "array",
 "property": "features",
 "idProperty": "properties.station"
 },
 "stations": {
 "type": "array",
 "property": "features",
 "identification": {
 "property": "properties.station"
 },
 "location": {
 "property": "geometry.coordinates",
 "type": "array",
 "longitude": "0",
 "latitude": "1"
 }
 },
 "datatypes": [
 {
 "datatypeName": "air_pressure",
 "method": "extract",
 "unitProperty": "properties.parameters.P.unit",
 "valueProperty": "properties.parameters.P.data",
 "targetUnit": "hPa",
 "rtype": "mean"
 },
 {
 "datatypeName": "wind_speed",
 "method": "extract",
 "unitProperty": "properties.parameters.FF.unit",
 "valueProperty": "properties.parameters.FF.data",
 "targetUnit": "m/s",
 "rtype": "mean"
 },
 {
 "datatypeName": "wind_direction",
 "method": "extract",
 "unitProperty": "properties.parameters.DD.unit",
 "valueProperty": "properties.parameters.DD.data",
 "targetUnit": "°",
 "rtype": "mean"
 },
 {
 "datatypeName": "wind_temperature",
 "method": "extract",
 "unitProperty": "properties.parameters.TL.unit",
 "valueProperty": "properties.parameters.TL.data",
 "targetUnit": "°F",
 "rtype": "mean"
 }
 ]
 }
}
  \end{lstlisting}
  
    \caption{Structural metadata for Austrian Geosphere dataset.\label{prog:metadata}}
\end{program}

The content describes how the data structure and in which keys are key values for the data transformation, for example: the timestamp of the event, the event location, and the value registered.

\subsection{Transformation}
\label{sec:transformation}

The data transformation is done by the Orchestrator, once it receives data with its metadata configuration. The Orchestrator uses the metadata to understand the particularities of the raw data and transform it into the ODH's station structure.

The Orchestrator is not only responsible for transforming the data structure to match the expected structure but also for transforming the collected value into the expected unit of measurement. Since the data is stored in the same unit to ease end users. It can do so, by using the Transformers.

Transformers are small pieces of code that do only one transformation into the value. For example, there may be a Transformer to transform a temperature value measured in Celsius to Kelvin. The Orchestrator is responsible for selecting the correct Transformers and running them into the correct order to transform from the collected unit to the one used by ODH's system. (Figure~\ref{fig:orchestrator})

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{collector-consumer-COMPONENT ORCHESTRATOR.drawio}
    \caption{Orchestrator components diagram.\label{fig:orchestrator}}
\end{figure}

\section{Experiment}
\label{sec:exp}

There is an experiment to test the proposed architecture. It simulates the integration of an Austrian geosphere data source, it ingests data on current air pressure, temperature, wind speed, and direction in different points from Austria. The data is collected every 10 minutes, the time the data source takes to update its values. This experiment aims to only simulate the data ingestion process and not send data to be persisted in the ODH system, but it allows observing the products of data ingestion and how each microservice relates and communicates with each other.

The experiment is in a GitHub repository as a Gradle monorepo, each service is a submodule of the repository, but each one can be deployed individually as a microservice, they are in Java 17. They use SpringBoot to create HTTP APIs and asynchronous workers. For infrastructure, it uses MongoDB for data storage and RabbitMQ for asynchronous communication via message queues.

To run the experiment, it is necessary to first prepare the environment, then run the infrastructure; after that, run the microservices, and finally include test metadata. It is also possible to simulate an active data source and send data directly to it to be ingested. This process is described step-by-step on the repository's README.

To access the HTTP endpoints, it is possible to use the Postman collection available in the repository, to act as an interface for debugging, active data ingestion, and/or metadata configuration.

During the experiment run, the applications logs are directed to files in the log folder to ease debugging and tracking. It is also possible to access the MongoDB, to track the data stored, and the RabbitMQ, to track messages and queues, servers by using the values in the docker-compose file.

\section{Takeaways}
\label{sec:conc}

It is possible to evaluate the extensibility of a data ingestion system by how easy it is to integrate different data sources and give maintenance to it.

On the current architecture, integrating a different data source would require creating a new application that would collect the data, transform and send it to ODH. Every time the data changes, it would require a change in the code base, this requires effort from the ODH's technical team and alignment between ODH and the data source owners.

The proposed architecture uses microservices and the concept of modularity to split the responsibility between the systems in different microservices. Also uses structural metadata to abstract transformation logic from the data collector.

For these reasons, integrating a new data source on the proposed architecture requires less technical work, once it is possible to be integrated by the own data source, via the ingestion API, or requires a data collector service that only contains code related to data collecting.

Also, maintaining the data sources is much easier, since data changes can be instantly met in the transformation process by changing the metadata configuration, which can be done by the data source team, via the metadata service API, reducing ODH technical team involvement.

Another advantage brought by the proposed architecture is the use of asynchronous communication. This reduces coupling problems, once the message publisher doesn't have any dependency with the consumer application. It also increases scalability, by reducing the harm of overload scenarios, since the messages are stored and only processed by the consumer once it is available.

A further improvement of the proposed architecture is the split of ingestion and transformation flows. Ingestion is the process of collecting the data and persisting it in the Raw Data Storage, and transformation is the process of using metadata to transform raw data into ODH's standardized data. Separating them means that data can be ingested even though they are not ready to be transformed (doesn't have a metadata configuration). This increases the data collecting capability, since it is not attached to the transformation logic, and reduces risks of data loss, once the data is stored and is going to be processed once the metadata is configured.

This also prevents unexpected behaviors on transformation from causing data loss, because the raw data is stored, and once the cause is found and fixed, the transformation can be triggered again.

Although there are many advantages to the proposed architecture, the use of asynchronous communication, microservices, and the addition of two data storage cause an increase in infrastructure cost and maintainability. For this reason, it is important to be sure there is an experienced team with microservices and the use of observability and tracking tools, such as Grafana, Prometheus, and Sentry, to make sure that the applications are healthy and working as intended and any unwanted behavior is caught and fixed as soon as possible.
